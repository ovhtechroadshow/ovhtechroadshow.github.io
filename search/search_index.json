{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Dokumentacja techniczna do warsztat\u00f3w PublicCloud w OVHcloud Witamy na stronie dokumentacji technicznej do warsztat\u00f3w OVHcloud TechRoadShow. G\u0142\u00f3wna strona OVHcloud TechRoadShow: https://techroadshow.ovh/ Materia\u0142y Ustawianie \u015brodowiska Podstawowe operacje na maszynie wirtualnej vRack Przerwa Horizon OpenStack CLI Swift Managed Kubernetes Service Przerwa CloudDB NAS-HA Tools","title":"Wprowadzenie"},{"location":"#dokumentacja-techniczna-do-warsztatow-publiccloud-w-ovhcloud","text":"Witamy na stronie dokumentacji technicznej do warsztat\u00f3w OVHcloud TechRoadShow. G\u0142\u00f3wna strona OVHcloud TechRoadShow: https://techroadshow.ovh/","title":"Dokumentacja techniczna do warsztat\u00f3w PublicCloud w OVHcloud"},{"location":"#materiay","text":"Ustawianie \u015brodowiska Podstawowe operacje na maszynie wirtualnej vRack Przerwa Horizon OpenStack CLI Swift Managed Kubernetes Service Przerwa CloudDB NAS-HA Tools","title":"Materia\u0142y"},{"location":"basic/","text":"Podstawowe operacje na maszynie wirtualnej W tej sekcji uruchomimy now\u0105 maszyn\u0119 wirtualn\u0105, zalogujemy si\u0119 na ni\u0105 i przeprowadzimy podstawowe operacje. 1. Utworzenie instancji z dockerem W Managerze w sekcji Compute w karcie Instances klikamy przycisk Utw\u00f3rz instancj\u0119 . W sekcji \"wybierz model\" wybieramy Discovery D2-2 - instancja z publicznym IP, 1 CPU i 2GB RAM. Lokalizacja: dowolna. System operacyjny: Ubuntu 22.04 Wybieramy klucz SSH , kt\u00f3ry wcze\u015bniej dodali\u015bmy. Konfiguracja instancji: w skrypcie poinstalacyjnym podajemy #include https://get.docker.com/ Rozliczenie: godzinowe 2. Logowanie si\u0119 do nowo utworzonej maszyny wirtualnej Linux, MacOS Windows Wykorzystamy klienta OpenSSH ssh -i ~/roadshow-workspace/key ubuntu@ ${ IP } Na pytanie The authenticity of host can't be established. Are you sure you want to continue connecting odpowiadamy: yes Wykorzystamy program putty.exe W programie putty w sekcji session podajemy adres IP. W sekcji Connection -> SSH -> Auth podajemy \u015bcie\u017ck\u0119 do pliku z kluczem prywatnym priv.ppk . Na pytanie czy na pewno jest to host do kt\u00f3rego chcemy si\u0119 po\u0142\u0105czy\u0107 odpowiadamy Accept . Podajemy nazw\u0119 u\u017cytkownika, kt\u00f3\u00a9ego chcemy u\u017cy\u0107 do logowania. W naszym przypadku jest to ubuntu . 3. Sprawdzenie parametr\u00f3w maszyny wirtualnej. \u017beby sprawdzi\u0107 podstawowe parametry maszyny wirtualnej u\u017cyjemy kilku podstawowych komend. Aby wy\u015bwietli\u0107 ilo\u015b\u0107 pami\u0119ci RAM: free Aby wy\u015bwietli\u0107 liczb\u0119 rdzeni procesora: nproc Aby wy\u015bwietli\u0107 urz\u0105dzenia blokowe: lsblk 4. Dodanie dodatkowego dysku do instancji Dodatkowy dysk do instancji dodajemy przez Managera. W sekcji Storage wchodzimy w zak\u0142adk\u0119 Block storage i klikamy przycisk Create a volume Ze wzgl\u0119du na ograniczenia geograficzne wybieramy t\u0105 sam\u0105 lokalizacj\u0119 jak ta, w kt\u00f3rej znajduj\u0105 si\u0119 maszyny wirtualne. Po utworzeniu klikamy trzy kropki po prawej stronie i wybieramy Attach to instance Po pod\u0142\u0105czeniu jeszcze raz wykonujemy polecenie lsblk . Przygotowanie dysku w systemie Linux Tak dodany dysk jest surowy wi\u0119c tworzymy na nim: tablic\u0119 partycji: sudo parted -s /dev/sdX mklabel gpt partycj\u0119: sudo parted -s /dev/sdX mkpart data 0% 100% system plik\u00f3w: sudo mkfs.ext4 /dev/sdX1 zamontowanie dysku sudo mount /dev/sdX1 /mnt Sprawdzenie wydajno\u015bci dysku Instalujemy oprogramowanie fio sudo apt update sudo apt -y install fio Przechodzimy do katalogu /mnt cd /mnt Uruchamiamy kr\u00f3tki test fio: sudo fio --name=test --bs=4k --ioengine=libaio --iodepth=4 --size=1g --direct=1 --runtime=30 Uruchomienie kontenera docker z nginx Aby uruchomi\u0107 kontener z serwerem nginx i opublikowa\u0107 go na porcie 80 wykonujemy komend\u0119: sudo docker run -d -p 80:80 --name nginx nginx:latest","title":"Podstawowe operacje na maszynie wirtualnej"},{"location":"basic/#podstawowe-operacje-na-maszynie-wirtualnej","text":"W tej sekcji uruchomimy now\u0105 maszyn\u0119 wirtualn\u0105, zalogujemy si\u0119 na ni\u0105 i przeprowadzimy podstawowe operacje.","title":"Podstawowe operacje na maszynie wirtualnej"},{"location":"basic/#1-utworzenie-instancji-z-dockerem","text":"W Managerze w sekcji Compute w karcie Instances klikamy przycisk Utw\u00f3rz instancj\u0119 . W sekcji \"wybierz model\" wybieramy Discovery D2-2 - instancja z publicznym IP, 1 CPU i 2GB RAM. Lokalizacja: dowolna. System operacyjny: Ubuntu 22.04 Wybieramy klucz SSH , kt\u00f3ry wcze\u015bniej dodali\u015bmy. Konfiguracja instancji: w skrypcie poinstalacyjnym podajemy #include https://get.docker.com/ Rozliczenie: godzinowe","title":"1. Utworzenie instancji z dockerem"},{"location":"basic/#2-logowanie-sie-do-nowo-utworzonej-maszyny-wirtualnej","text":"Linux, MacOS Windows Wykorzystamy klienta OpenSSH ssh -i ~/roadshow-workspace/key ubuntu@ ${ IP } Na pytanie The authenticity of host can't be established. Are you sure you want to continue connecting odpowiadamy: yes Wykorzystamy program putty.exe W programie putty w sekcji session podajemy adres IP. W sekcji Connection -> SSH -> Auth podajemy \u015bcie\u017ck\u0119 do pliku z kluczem prywatnym priv.ppk . Na pytanie czy na pewno jest to host do kt\u00f3rego chcemy si\u0119 po\u0142\u0105czy\u0107 odpowiadamy Accept . Podajemy nazw\u0119 u\u017cytkownika, kt\u00f3\u00a9ego chcemy u\u017cy\u0107 do logowania. W naszym przypadku jest to ubuntu .","title":"2. Logowanie si\u0119\u00a0do nowo utworzonej maszyny wirtualnej"},{"location":"basic/#3-sprawdzenie-parametrow-maszyny-wirtualnej","text":"\u017beby sprawdzi\u0107 podstawowe parametry maszyny wirtualnej u\u017cyjemy kilku podstawowych komend. Aby wy\u015bwietli\u0107 ilo\u015b\u0107 pami\u0119ci RAM: free Aby wy\u015bwietli\u0107 liczb\u0119 rdzeni procesora: nproc Aby wy\u015bwietli\u0107 urz\u0105dzenia blokowe: lsblk","title":"3. Sprawdzenie parametr\u00f3w maszyny wirtualnej."},{"location":"basic/#4-dodanie-dodatkowego-dysku-do-instancji","text":"Dodatkowy dysk do instancji dodajemy przez Managera. W sekcji Storage wchodzimy w zak\u0142adk\u0119 Block storage i klikamy przycisk Create a volume Ze wzgl\u0119du na ograniczenia geograficzne wybieramy t\u0105 sam\u0105 lokalizacj\u0119 jak ta, w kt\u00f3rej znajduj\u0105 si\u0119 maszyny wirtualne. Po utworzeniu klikamy trzy kropki po prawej stronie i wybieramy Attach to instance Po pod\u0142\u0105czeniu jeszcze raz wykonujemy polecenie lsblk .","title":"4. Dodanie dodatkowego dysku do instancji"},{"location":"basic/#przygotowanie-dysku-w-systemie-linux","text":"Tak dodany dysk jest surowy wi\u0119c tworzymy na nim: tablic\u0119 partycji: sudo parted -s /dev/sdX mklabel gpt partycj\u0119: sudo parted -s /dev/sdX mkpart data 0% 100% system plik\u00f3w: sudo mkfs.ext4 /dev/sdX1 zamontowanie dysku sudo mount /dev/sdX1 /mnt","title":"Przygotowanie dysku w systemie Linux"},{"location":"basic/#sprawdzenie-wydajnosci-dysku","text":"Instalujemy oprogramowanie fio sudo apt update sudo apt -y install fio Przechodzimy do katalogu /mnt cd /mnt Uruchamiamy kr\u00f3tki test fio: sudo fio --name=test --bs=4k --ioengine=libaio --iodepth=4 --size=1g --direct=1 --runtime=30","title":"Sprawdzenie wydajno\u015bci dysku"},{"location":"basic/#uruchomienie-kontenera-docker-z-nginx","text":"Aby uruchomi\u0107 kontener z serwerem nginx i opublikowa\u0107 go na porcie 80 wykonujemy komend\u0119: sudo docker run -d -p 80:80 --name nginx nginx:latest","title":"Uruchomienie kontenera docker z nginx"},{"location":"cli/","text":"Openstack CLI Note Na tym etapie korzystamy z utworzonych instancji, \u017ceby u\u0142atwi\u0107 sobie prac\u0119. 1. Instalacja klienta Openstack Logujemy si\u0119 po SSH do instancji. Na instancji wykonujemy polecenia: sudo apt update sudo apt install -y python3-pip python3-dev sudo pip3 install --upgrade pip sudo pip3 install python-openstackclient 2. U\u017cycie danych do logowania Pobieramy plik RC OpenStack z panelu OVHcloud. Warning Plik pobieramy dla regionu, gdzie istnieje nasza instancja. Na przyk\u0142ad: Kopiujemy plik do instancji. Linux, MacOS Windows scp -i ~/roadshow-workspace/key openrc ubuntu@<public_ip>:~/ Kopiujemy zawarto\u015b\u0107 pliku przy u\u017cyciu schowka do pliku ~/openrc na instancji. U\u017cywamy pobranego pliku do po\u0142\u0105czenia si\u0119 z OpenStack API. source openrc Wywo\u0142anie poprzedniej komendy spowoduje uruchomienie prompta. Do logowania u\u017cyjemy has\u0142a zapisanego podczas tworzenia u\u017cytkownika w panelu OVHcloud. 3. Zablokowanie instancji Znajdujemy id naszej instancji przy u\u017cyciu: openstack server list Przy pomocy komendy blokujemy operacje na instancji: openstack server lock <vm_id> Upewniamy si\u0119, \u017ce maszyna wirtualna nie mo\u017ce zosta\u0107 usuni\u0119ta z poziomu panelu OVHcloud. Zwalniamy blokad\u0119 dla wybranej instancji. openstack server unlock <vm_id> Usuwamy instancj\u0119 poprzez panel OVHcloud.","title":"Openstack CLI"},{"location":"cli/#openstack-cli","text":"Note Na tym etapie korzystamy z utworzonych instancji, \u017ceby u\u0142atwi\u0107 sobie prac\u0119.","title":"Openstack CLI"},{"location":"cli/#1-instalacja-klienta-openstack","text":"Logujemy si\u0119 po SSH do instancji. Na instancji wykonujemy polecenia: sudo apt update sudo apt install -y python3-pip python3-dev sudo pip3 install --upgrade pip sudo pip3 install python-openstackclient","title":"1. Instalacja klienta Openstack"},{"location":"cli/#2-uzycie-danych-do-logowania","text":"Pobieramy plik RC OpenStack z panelu OVHcloud. Warning Plik pobieramy dla regionu, gdzie istnieje nasza instancja. Na przyk\u0142ad: Kopiujemy plik do instancji. Linux, MacOS Windows scp -i ~/roadshow-workspace/key openrc ubuntu@<public_ip>:~/ Kopiujemy zawarto\u015b\u0107 pliku przy u\u017cyciu schowka do pliku ~/openrc na instancji. U\u017cywamy pobranego pliku do po\u0142\u0105czenia si\u0119 z OpenStack API. source openrc Wywo\u0142anie poprzedniej komendy spowoduje uruchomienie prompta. Do logowania u\u017cyjemy has\u0142a zapisanego podczas tworzenia u\u017cytkownika w panelu OVHcloud.","title":"2. U\u017cycie danych do logowania"},{"location":"cli/#3-zablokowanie-instancji","text":"Znajdujemy id naszej instancji przy u\u017cyciu: openstack server list Przy pomocy komendy blokujemy operacje na instancji: openstack server lock <vm_id> Upewniamy si\u0119, \u017ce maszyna wirtualna nie mo\u017ce zosta\u0107 usuni\u0119ta z poziomu panelu OVHcloud. Zwalniamy blokad\u0119 dla wybranej instancji. openstack server unlock <vm_id> Usuwamy instancj\u0119 poprzez panel OVHcloud.","title":"3. Zablokowanie instancji"},{"location":"clouddb/","text":"CloudDB PostgreSQL 1. Pod\u0142\u0105czenie do bazy danych Instalujemy klienta postgresql: sudo apt-get install postgresql-14 Sprawdzamy zainstalowan\u0105 wersj\u0119: psql --version Pod\u0142\u0105czamy si\u0119 do bazy danych: psql \"postgres://username:password@hostname:port/defaultdb?sslmode=require\" Obs\u0142uga bazy danych Przyk\u0142adowe polecenia Lista tabel \\dt Tworzenie tabeli CREATE TABLE tabela (first TEXT, last TEXT); Listowanie zawarto\u015bci tabeli SELECT * FROM tabela; Kasowanie tabeli DROP TABLE tabela;","title":"CloudDB PostgreSQL"},{"location":"clouddb/#clouddb-postgresql","text":"","title":"CloudDB PostgreSQL"},{"location":"clouddb/#1-podaczenie-do-bazy-danych","text":"Instalujemy klienta postgresql: sudo apt-get install postgresql-14 Sprawdzamy zainstalowan\u0105 wersj\u0119: psql --version Pod\u0142\u0105czamy si\u0119 do bazy danych: psql \"postgres://username:password@hostname:port/defaultdb?sslmode=require\"","title":"1. Pod\u0142\u0105czenie do bazy danych"},{"location":"clouddb/#obsuga-bazy-danych","text":"","title":"Obs\u0142uga bazy danych"},{"location":"clouddb/#przykadowe-polecenia","text":"Lista tabel \\dt Tworzenie tabeli CREATE TABLE tabela (first TEXT, last TEXT); Listowanie zawarto\u015bci tabeli SELECT * FROM tabela; Kasowanie tabeli DROP TABLE tabela;","title":"Przyk\u0142adowe polecenia"},{"location":"env/","text":"Ustawianie \u015brodowiska W tej sekcji przygotujemy podstawowe \u015brodowisko do pracy 1. Stworzenie folderu roboczego Linux, MacOS Windows mkdir ~/roadshow-workspace cd ~/roadshow-workspace Utworzy\u0107 na pulpicie folder roadshow-workspace 2. Stworzenie pary kluczy SSH Stworzymy par\u0119 kluczy typu ED25519 dla zgodno\u015bci z nowymi systemami operacyjnymi. Linux, MacOS Windows Wykorzystamy polecenie ssh-keygen z OpenSSH ssh-keygen -t ed25519 -f ~/roadshow-workspace/key Wykorzystamy program puttygen.exe kt\u00f3ry mo\u017cna pobra\u0107 z oficjalnej strony projektu: https://www.chiark.greenend.org.uk/~sgtatham/putty/latest.html Wygenerujemy par\u0119 kluczy SSH Zapiszemy klucz publiczny do katalogu roboczego Zapiszemy klucz prywatny bez has\u0142a do katalogu roboczego 3. Odczytanie klucza publicznego Najpierw musimy odczyta\u0107 cz\u0119\u015b\u0107 publiczn\u0105 klucza SSH. Linux, MacOS Windows cat ~/roadshow-workspace/key.pub Skopiujemy do schowka ca\u0142y ci\u0105g znak\u00f3w ssh-ed25519 KEEEY user@host . To jest nasz klucz publiczny. W programie puttygen.exe przez przycisk Load wczytamy plik priv z folderu roadshow-workspace . Nast\u0119pnie skopiujemy ca\u0142y ci\u0105g znak\u00f3w z okienka Public key for pasting into OpenSSH authorized_keys file . To jest nasz klucz publiczny. 4. Dodanie klucza do Managera (panelu OVHcloud) Teraz wejdziemy do panelu OVHcloud do zak\u0142adki Public Cloud. Wybieramy sw\u00f3j projekt i w sekcji Project Management wybieramy SSH Keys. Nast\u0119pnie klikamy na Dodaj klucz SSH i podajemy nazw\u0119 oraz wklejamy cz\u0119\u015b\u0107 publiczn\u0105 klucza","title":"Ustawianie \u015brodowiska"},{"location":"env/#ustawianie-srodowiska","text":"W tej sekcji przygotujemy podstawowe \u015brodowisko do pracy","title":"Ustawianie \u015brodowiska"},{"location":"env/#1-stworzenie-folderu-roboczego","text":"Linux, MacOS Windows mkdir ~/roadshow-workspace cd ~/roadshow-workspace Utworzy\u0107 na pulpicie folder roadshow-workspace","title":"1. Stworzenie folderu roboczego"},{"location":"env/#2-stworzenie-pary-kluczy-ssh","text":"Stworzymy par\u0119 kluczy typu ED25519 dla zgodno\u015bci z nowymi systemami operacyjnymi. Linux, MacOS Windows Wykorzystamy polecenie ssh-keygen z OpenSSH ssh-keygen -t ed25519 -f ~/roadshow-workspace/key Wykorzystamy program puttygen.exe kt\u00f3ry mo\u017cna pobra\u0107 z oficjalnej strony projektu: https://www.chiark.greenend.org.uk/~sgtatham/putty/latest.html Wygenerujemy par\u0119 kluczy SSH Zapiszemy klucz publiczny do katalogu roboczego Zapiszemy klucz prywatny bez has\u0142a do katalogu roboczego","title":"2. Stworzenie pary kluczy SSH"},{"location":"env/#3-odczytanie-klucza-publicznego","text":"Najpierw musimy odczyta\u0107 cz\u0119\u015b\u0107 publiczn\u0105 klucza SSH. Linux, MacOS Windows cat ~/roadshow-workspace/key.pub Skopiujemy do schowka ca\u0142y ci\u0105g znak\u00f3w ssh-ed25519 KEEEY user@host . To jest nasz klucz publiczny. W programie puttygen.exe przez przycisk Load wczytamy plik priv z folderu roadshow-workspace . Nast\u0119pnie skopiujemy ca\u0142y ci\u0105g znak\u00f3w z okienka Public key for pasting into OpenSSH authorized_keys file . To jest nasz klucz publiczny.","title":"3. Odczytanie klucza publicznego"},{"location":"env/#4-dodanie-klucza-do-managera-panelu-ovhcloud","text":"Teraz wejdziemy do panelu OVHcloud do zak\u0142adki Public Cloud. Wybieramy sw\u00f3j projekt i w sekcji Project Management wybieramy SSH Keys. Nast\u0119pnie klikamy na Dodaj klucz SSH i podajemy nazw\u0119 oraz wklejamy cz\u0119\u015b\u0107 publiczn\u0105 klucza","title":"4. Dodanie klucza do Managera (panelu OVHcloud)"},{"location":"horizon/","text":"Horizon 1. Utworzenie u\u017cytkownika Openstack Przechodzimy do \"Project Management -> Users & Roles\" Tworzymy u\u017cytkownika z uprawnieniami Administratora Kopiujemy has\u0142o Warning Has\u0142o jest wy\u015bwietlane tylko raz, dlatego prosz\u0119 je zapisa\u0107 2. Obs\u0142uga panelu Horizon Przechodzimy do panelu klienta Horizon Korzystaj\u0105c z loginu oraz has\u0142a z punktu 1 logujemy si\u0119 do panelu Note Login powinien wygl\u0105da\u0107 nast\u0119puj\u0105co: user-XXXXXXXXX 3. Security Groups Sprawdzamy jakie security groupy s\u0105 u\u017cywane przez instancje. Aby to zrobi\u0107 przechodzimy do zak\u0142adki \"Compute -> Instances\". Nast\u0119pnie w g\u00f3rnym panelu wybieramy odpowiedni region. Note Wszystkie instancje na pocz\u0105tku korzystaj\u0105 z domy\u015blnych ustawie\u0144, wi\u0119c pozwalaj\u0105 na ka\u017cdy rodzaj ruchu sieciowego. Przechodzimy do zak\u0142adki \"Network -> Security Groups\" ( https://horizon.cloud.ovh.net/project/security_groups/ ) i klikamy \"Manage Rules\" Modyfikujemy domy\u015blne regu\u0142y tak, \u017ceby nie zosta\u0142y \u017cadne regu\u0142y dotycz\u0105ce IPv4 (je\u017celi nie mamy zamiaru u\u017cywa\u0107 IPv6 mo\u017cna powi\u0105zane z nimi grupy r\u00f3wnie\u017c usun\u0105\u0107) Sprawdzamy po\u0142\u0105czenie do instancji Dodajemy nowe zasady dotycz\u0105ce security group, aby pozwoli\u0107 na ruch po podstawowych protoko\u0142ach: SSH, HTTPS, HTTP Ponownie sprawdzamy po\u0142\u0105czenie (Opcjonalnie) Mo\u017cemy sprawdzi\u0107 czy nasz kontener z serwisem Nginx jeszcze dzia\u0142a","title":"Horizon"},{"location":"horizon/#horizon","text":"","title":"Horizon"},{"location":"horizon/#1-utworzenie-uzytkownika-openstack","text":"Przechodzimy do \"Project Management -> Users & Roles\" Tworzymy u\u017cytkownika z uprawnieniami Administratora Kopiujemy has\u0142o Warning Has\u0142o jest wy\u015bwietlane tylko raz, dlatego prosz\u0119 je zapisa\u0107","title":"1. Utworzenie u\u017cytkownika Openstack"},{"location":"horizon/#2-obsuga-panelu-horizon","text":"Przechodzimy do panelu klienta Horizon Korzystaj\u0105c z loginu oraz has\u0142a z punktu 1 logujemy si\u0119 do panelu Note Login powinien wygl\u0105da\u0107 nast\u0119puj\u0105co: user-XXXXXXXXX","title":"2. Obs\u0142uga panelu Horizon"},{"location":"horizon/#3-security-groups","text":"Sprawdzamy jakie security groupy s\u0105 u\u017cywane przez instancje. Aby to zrobi\u0107 przechodzimy do zak\u0142adki \"Compute -> Instances\". Nast\u0119pnie w g\u00f3rnym panelu wybieramy odpowiedni region. Note Wszystkie instancje na pocz\u0105tku korzystaj\u0105 z domy\u015blnych ustawie\u0144, wi\u0119c pozwalaj\u0105 na ka\u017cdy rodzaj ruchu sieciowego. Przechodzimy do zak\u0142adki \"Network -> Security Groups\" ( https://horizon.cloud.ovh.net/project/security_groups/ ) i klikamy \"Manage Rules\" Modyfikujemy domy\u015blne regu\u0142y tak, \u017ceby nie zosta\u0142y \u017cadne regu\u0142y dotycz\u0105ce IPv4 (je\u017celi nie mamy zamiaru u\u017cywa\u0107 IPv6 mo\u017cna powi\u0105zane z nimi grupy r\u00f3wnie\u017c usun\u0105\u0107) Sprawdzamy po\u0142\u0105czenie do instancji Dodajemy nowe zasady dotycz\u0105ce security group, aby pozwoli\u0107 na ruch po podstawowych protoko\u0142ach: SSH, HTTPS, HTTP Ponownie sprawdzamy po\u0142\u0105czenie (Opcjonalnie) Mo\u017cemy sprawdzi\u0107 czy nasz kontener z serwisem Nginx jeszcze dzia\u0142a","title":"3. Security Groups"},{"location":"k8s/","text":"Managed Kubernetes Service 1. Utworzenie ma\u0142ego klastra Kubernetes (3 w\u0119z\u0142y) 2. Instalacja narz\u0119dzia kubectl - instrukcja 3. Stworzenie po\u0142\u0105czenia z klastrem Pobieramy plik kubeconfig z panelu OVHcloud Kopiujemy plik do instancji Linux, MacOS Windows scp -i ~/roadshow-workspace/key kubeconfig.yml ubuntu@<public_ip>:~/ Kopiujemy zawarto\u015b\u0107 pliku przy u\u017cyciu schowka do pliku ~/kubeconfig.yml na instancji. U\u017cywamy konfiguracji do po\u0142\u0105czenia z klastrem export KUBECONFIG=~/kubeconfig.yml 4. Stworzenie nowego serwisu Listujemy w\u0119z\u0142y klastra kubectl get nodes Tworzymy Poda z obrazem serwera Nginx kubectl run demo-nginx --image=nginx --port 80 Publikujemy stron\u0119 w Internecie kubectl expose pod demo-nginx --type=LoadBalancer Szukamy serwisu LoadBalancer stworzonego dla naszego Poda kubectl get service demo-nginx Sprawdzamy czy strona jest dost\u0119pna w Internecie","title":"Managed Kubernetes Service"},{"location":"k8s/#managed-kubernetes-service","text":"","title":"Managed Kubernetes Service"},{"location":"k8s/#1-utworzenie-maego-klastra-kubernetes-3-wezy","text":"","title":"1. Utworzenie ma\u0142ego klastra Kubernetes (3 w\u0119z\u0142y)"},{"location":"k8s/#2-instalacja-narzedzia-kubectl-instrukcja","text":"","title":"2. Instalacja narz\u0119dzia kubectl - instrukcja"},{"location":"k8s/#3-stworzenie-poaczenia-z-klastrem","text":"Pobieramy plik kubeconfig z panelu OVHcloud Kopiujemy plik do instancji Linux, MacOS Windows scp -i ~/roadshow-workspace/key kubeconfig.yml ubuntu@<public_ip>:~/ Kopiujemy zawarto\u015b\u0107 pliku przy u\u017cyciu schowka do pliku ~/kubeconfig.yml na instancji. U\u017cywamy konfiguracji do po\u0142\u0105czenia z klastrem export KUBECONFIG=~/kubeconfig.yml","title":"3. Stworzenie po\u0142\u0105czenia z klastrem"},{"location":"k8s/#4-stworzenie-nowego-serwisu","text":"Listujemy w\u0119z\u0142y klastra kubectl get nodes Tworzymy Poda z obrazem serwera Nginx kubectl run demo-nginx --image=nginx --port 80 Publikujemy stron\u0119 w Internecie kubectl expose pod demo-nginx --type=LoadBalancer Szukamy serwisu LoadBalancer stworzonego dla naszego Poda kubectl get service demo-nginx Sprawdzamy czy strona jest dost\u0119pna w Internecie","title":"4. Stworzenie nowego serwisu"},{"location":"nasha/","text":"NAS-HA Jak pod\u0142\u0105czy\u0107 wolumen NAS-HA Linux Zainstaluj klienta NFS: sudo apt-get install nfs-common Utw\u00f3rz nowy katalog: sudo mkdir /media/NasHA Zamontuj wolumen serwera plik\u00f3w: sudo mount -t nfs -o _netdev,mountproto=tcp 10.201.27.176:zpool-128495/RoadShow /media/NasHA Windows Uruchom PowerShell albo konsol\u0119 i wykonaj komend\u0119: net use z: \\\\10.201.27.176\\zpool-128495_RoadShow","title":"NAS-HA"},{"location":"nasha/#nas-ha","text":"","title":"NAS-HA"},{"location":"nasha/#jak-podaczyc-wolumen-nas-ha","text":"","title":"Jak pod\u0142\u0105czy\u0107 wolumen NAS-HA"},{"location":"nasha/#linux","text":"Zainstaluj klienta NFS: sudo apt-get install nfs-common Utw\u00f3rz nowy katalog: sudo mkdir /media/NasHA Zamontuj wolumen serwera plik\u00f3w: sudo mount -t nfs -o _netdev,mountproto=tcp 10.201.27.176:zpool-128495/RoadShow /media/NasHA","title":"Linux"},{"location":"nasha/#windows","text":"Uruchom PowerShell albo konsol\u0119 i wykonaj komend\u0119: net use z: \\\\10.201.27.176\\zpool-128495_RoadShow","title":"Windows"},{"location":"swift/","text":"Swift 1. Utworzenie publicznego bucketu/kontenera S3 o nazwie demo-container w panelu OVHcloud 2. Wys\u0142anie pliku Utworzenie pliku file.txt z tekstem hello world w folderze roadshow-workspace Wys\u0142anie pliku do stworzonego kontenera Sprawdzenie, \u017ce plik istnieje w danym kontenerze w przegl\u0105darce. Note Plik b\u0119dzie dost\u0119pny pod adresem kt\u00f3ry ma struktur\u0119: <endpoint>/file.txt 3. Pobranie pliku U\u017cywamy tego samego adresu URL do \u015bci\u0105gni\u0119cia pliku na instancj\u0119 curl <endpoint>/file.txt > ~/file.txt cat ~/file.txt 4. Po\u0142\u0105czenie do bucketu z poziomu klienta Pobranie i instalacja klienta MinIO: https://min.io/docs/minio/linux/reference/minio-mc.html#install-mc Utworzenie u\u017cytkownika S3 w panelu OVHcloud Skonfigurowanie klienta tak, \u017ceby by\u0142 po\u0142\u0105czony do naszego Object Storage. mc alias set demo <endpoint> <accessKey> <secretKey> Wylistowanie istniej\u0105cych bucket\u00f3w mc ls demo Wylistowanie obiekt\u00f3w z kontenera/bucketu mc ls demo/demo-container Pobranie obiektu z kontenera mc cp demo/demo-container/file.txt ~/ cat ~/file.txt","title":"Swift"},{"location":"swift/#swift","text":"","title":"Swift"},{"location":"swift/#1-utworzenie-publicznego-bucketukontenera-s3-o-nazwie-demo-container-w-panelu-ovhcloud","text":"","title":"1. Utworzenie publicznego bucketu/kontenera S3 o nazwie demo-container w panelu OVHcloud"},{"location":"swift/#2-wysanie-pliku","text":"Utworzenie pliku file.txt z tekstem hello world w folderze roadshow-workspace Wys\u0142anie pliku do stworzonego kontenera Sprawdzenie, \u017ce plik istnieje w danym kontenerze w przegl\u0105darce. Note Plik b\u0119dzie dost\u0119pny pod adresem kt\u00f3ry ma struktur\u0119: <endpoint>/file.txt","title":"2. Wys\u0142anie pliku"},{"location":"swift/#3-pobranie-pliku","text":"U\u017cywamy tego samego adresu URL do \u015bci\u0105gni\u0119cia pliku na instancj\u0119 curl <endpoint>/file.txt > ~/file.txt cat ~/file.txt","title":"3. Pobranie pliku"},{"location":"swift/#4-poaczenie-do-bucketu-z-poziomu-klienta","text":"Pobranie i instalacja klienta MinIO: https://min.io/docs/minio/linux/reference/minio-mc.html#install-mc Utworzenie u\u017cytkownika S3 w panelu OVHcloud Skonfigurowanie klienta tak, \u017ceby by\u0142 po\u0142\u0105czony do naszego Object Storage. mc alias set demo <endpoint> <accessKey> <secretKey> Wylistowanie istniej\u0105cych bucket\u00f3w mc ls demo Wylistowanie obiekt\u00f3w z kontenera/bucketu mc ls demo/demo-container Pobranie obiektu z kontenera mc cp demo/demo-container/file.txt ~/ cat ~/file.txt","title":"4. Po\u0142\u0105czenie do bucketu z poziomu klienta"},{"location":"tools/","text":"Przydatne narz\u0119dzia Lista przydatnych narz\u0119dzi dost\u0119pnych przez przegl\u0105dark\u0119 Topologia i obici\u0105\u017cenie sieci OVH Network Latency Grapher Punkty styku, IX, PoP, BGP Dzia\u0142ania serwisowe, awarie Visual Monitoring System Speedtest Who is? OVHcloud Labs","title":"Przydatne narz\u0119dzia"},{"location":"tools/#przydatne-narzedzia","text":"Lista przydatnych narz\u0119dzi dost\u0119pnych przez przegl\u0105dark\u0119 Topologia i obici\u0105\u017cenie sieci OVH Network Latency Grapher Punkty styku, IX, PoP, BGP Dzia\u0142ania serwisowe, awarie Visual Monitoring System Speedtest Who is? OVHcloud Labs","title":"Przydatne narz\u0119dzia"},{"location":"vrack/","text":"vRack 1. Utworzenie sieci vRack W Managerze w sekcji Network klikamy na Private Network. Poka\u017ce si\u0119 informacja, \u017ce projekt Public Cloud nie ma utworzonych jeszcze \u017cadnych sieci vRack, wi\u0119c tworzymy now\u0105 sie\u0107 klikaj\u0105c na przycisk Create vRack i potwierdzaj\u0105c operacj\u0119 w okienku popup. 2. Dodanie nowej sieci vRack Po utworzeniu nowej sieci vRack nale\u017cy dodajemy now\u0105 sie\u0107 prywatn\u0105. W tym celu klikamy na przycisk Add Private Network . Address distribution: ustawi\u0107 jako DHCP. Region: najlepiej wybra\u0107 wszystkie. Nazwa: dowolna. 3. Dodanie sieci prywatnej do VMki W sekcji Compute klikamy na Instances. Nast\u0119pnie klikamy w nazw\u0119 instancji kt\u00f3r\u0105 chcemy edytowa\u0107. Po prawej stronie w kolumnie Network w sekcji Private networks pod przyciskiem z trzema kropkami wybieramy Attach a network . 4. Podniesienie interfejsu vRack Aby zobaczy\u0107 wszystkie interfejsy sieciowe na VMce wykonujemy polecenie: ip link Aby podnie\u015b\u0107 interfejs vRack i uzyska\u0107 na nim adres IP przez DHCP: sudo dhclient ens7 Aby zmiany zosta\u0142y zachowane po restarcie mo\u017cna je doda\u0107 do konfiguracji sieciowej w /etc/netplan/ .","title":"vRack"},{"location":"vrack/#vrack","text":"","title":"vRack"},{"location":"vrack/#1-utworzenie-sieci-vrack","text":"W Managerze w sekcji Network klikamy na Private Network. Poka\u017ce si\u0119 informacja, \u017ce projekt Public Cloud nie ma utworzonych jeszcze \u017cadnych sieci vRack, wi\u0119c tworzymy now\u0105 sie\u0107 klikaj\u0105c na przycisk Create vRack i potwierdzaj\u0105c operacj\u0119 w okienku popup.","title":"1. Utworzenie sieci vRack"},{"location":"vrack/#2-dodanie-nowej-sieci-vrack","text":"Po utworzeniu nowej sieci vRack nale\u017cy dodajemy now\u0105 sie\u0107 prywatn\u0105. W tym celu klikamy na przycisk Add Private Network . Address distribution: ustawi\u0107 jako DHCP. Region: najlepiej wybra\u0107 wszystkie. Nazwa: dowolna.","title":"2. Dodanie nowej sieci vRack"},{"location":"vrack/#3-dodanie-sieci-prywatnej-do-vmki","text":"W sekcji Compute klikamy na Instances. Nast\u0119pnie klikamy w nazw\u0119 instancji kt\u00f3r\u0105 chcemy edytowa\u0107. Po prawej stronie w kolumnie Network w sekcji Private networks pod przyciskiem z trzema kropkami wybieramy Attach a network .","title":"3. Dodanie sieci prywatnej do VMki"},{"location":"vrack/#4-podniesienie-interfejsu-vrack","text":"Aby zobaczy\u0107 wszystkie interfejsy sieciowe na VMce wykonujemy polecenie: ip link Aby podnie\u015b\u0107 interfejs vRack i uzyska\u0107 na nim adres IP przez DHCP: sudo dhclient ens7 Aby zmiany zosta\u0142y zachowane po restarcie mo\u017cna je doda\u0107 do konfiguracji sieciowej w /etc/netplan/ .","title":"4. Podniesienie interfejsu vRack"}]}